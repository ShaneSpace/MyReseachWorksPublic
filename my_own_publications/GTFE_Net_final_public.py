import tensorflow as tf
import tensorflow.keras.layers as layers
import matplotlib.pyplot as plt
import numpy as np
import re


import scipy.io as sio
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score,recall_score, f1_score, roc_curve, roc_auc_score, auc
from sklearn.preprocessing import label_binarize
import pandas as pd


from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical


from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution
disable_eager_execution()



def cal_index(y_true, y_pred):
    '''
    Calculate Accuracy, Recall, Precision, F1-Score
    https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin
    '''
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, average='macro',labels=np.unique(y_pred))
    recall = recall_score(y_true, y_pred, average='macro',labels=np.unique(y_pred))
    F1_score = f1_score(y_true, y_pred, average='macro',labels=np.unique(y_pred))

    return acc, prec, recall, F1_score

def cal_auc_roc(y_true, y_pred_score, num_class=7):
    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html
    y_true_one_hot = label_binarize(y_true, classes=list(range(num_class)))

    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_class):
        fpr[i], tpr[i], _ = roc_curve(y_true_one_hot[:, i], y_pred_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])


    return roc_auc, fpr, tpr

#%%
def conv1d_blk(x,filters,kernel_size,strides,padding):
    x1 = layers.Conv1D(filters, kernel_size, strides, padding)(x)
    x2 = layers.BatchNormalization()(x1)
    x3 = layers.Activation('relu')(x2)
    x3 = layers.Dropout(0.2)(x3)

    return x3

def feature_map_reshape12(x):
    '''
    - Description: Covert the 1D feature map into 2D feature map
    - Input: x, shape:B*L*C, x is batch of 1D feature maps with C channels. - - Note that the length L should be 2^k, where k is positive integer
    - Output:y, shape: B*H*W*C

    Example:
    input_shape = (11, 512, 7)
    x = tf.random.normal(input_shape)
    output_shape: TensorShape([11, 16, 32, 7])
    '''

    x_shape = x.shape
    len_feature_map1d = x_shape[1]

    tmp = np.log2(len_feature_map1d)
    if int(tmp%2)==0:
        h = int(2**(tmp/2))
        w = int(len_feature_map1d/h)
    elif int(tmp%2)==1:
        h = int(2**((tmp-1)/2))
        w = int(len_feature_map1d/h)


    samples_1d = tf.transpose(x, perm = [0,2,1])
    feature_map_shape2d = [-1, x_shape[2], h, w]
    # feature_map_shape2d = [-1, x_shape[2], w, h] # there is no significant difference between the h-first and w-first

    samples_2d = tf.reshape(samples_1d, feature_map_shape2d)
    y = tf.transpose(samples_2d, perm =[0, 2, 3, 1])

    return y


def gram_blk(x):
    '''
    - Description: Calculate the Gram Enhanced feature map based on 2D feature map
    - Input: x, shape: B*H*W*C
    - Output: y = (x*x.T)*x*(x.T*x)
    '''
    xg1 = tf.einsum('bijc,bjkc->bikc',tf.transpose(x, perm=[0,2,1,3]), x)
    xg2 = tf.einsum('bijc,bjkc->bikc',x, tf.transpose(x, perm=[0,2,1,3]))

    y1 = tf.einsum('bijc,bjkc->bikc',xg2, x)
    y2 = tf.einsum('bijc,bjkc->bikc',y1, xg1)


    return y2


def feature_map_reshape21(x):
    '''
    - Description: Covert feature maps (generated by gram_blk) from 2D to 1D
    - Input: x, shape:B*H*W*C, x is batch of 2D feature maps with C channels.
    - Output:y, shape: B*L*C
    - Note that the length L should be 2^k, where k is positive integer


    Example:
    input_shape = [11, 16, 32, 7]
    x = tf.random.normal(input_shape)
    output_shape: TensorShape([11, 512, 7])
    '''

    x_shape = x.shape
    len_feature_map1d = x_shape[1]*x_shape[2]
    feature_map_shape2d = [-1, len_feature_map1d, x_shape[3] ]
    y = tf.reshape(x, feature_map_shape2d)

    return y

def batch_gram_blk(x):
    x1 = feature_map_reshape12(x)
    x2 = gram_blk(x1)
    x3 = feature_map_reshape21(x2)
    y = layers.LayerNormalization(axis=1)(x3)

    return y

def batch_fft_blk(x):
    data_length = x.shape[1]
    x1 = tf.transpose(x, perm=[0,2,1])
    x2 = tf.math.abs(tf.signal.rfft(x1))/data_length
    x3 = tf.transpose(x2, perm=[0,2,1])
    x4 = layers.LayerNormalization(axis=1)(x3)
    return x4


def the_blk(xr0, xg0, xf0, filters=16, kernel_size=3, strides=1, padding='same'):
    '''
    disconnect the Fourier bridges, only Gramian bridge is kept.
    '''
    xr1   = conv1d_blk(xr0,filters = filters, kernel_size = kernel_size, strides = strides, padding = padding)
    xr1_g = batch_gram_blk(xr1)
    xg0_c = conv1d_blk(xg0,filters = filters, kernel_size = kernel_size, strides = strides, padding = padding)
    xg1   = tf.concat([xr1_g,xg0_c], axis = 2)
    # xg1_0   = tf.concat([xr1_g,xg0_c], axis = 2)
    # xg1 = conv1d_blk(xg1_0, filters = filters, kernel_size = 1, strides = 1, padding = padding) #notice that

    xf1 = conv1d_blk(xf0,filters = filters, kernel_size = kernel_size, strides = strides, padding = padding)


    return xr1, xg1, xf1



def the_model(data_shape=(1024,1),class_number=10):
    '''
    Full Model: Three branches (raw signal Branch + Gramian branch + Fourier Branch)
    input_shape = (10, 1024,1)
    in_x = tf.random.normal(input_shape)
    '''
    # Bottom Block
    in_x = layers.Input(shape=data_shape)
    xr0  = in_x
    xg0  = batch_gram_blk(in_x)
    xg0_f  = batch_fft_blk(xg0)
    xr0_f  = batch_fft_blk(xr0)
    xf0    = tf.concat([xg0_f, xr0_f], axis = 2)

    xr1, xg1, xf1 = the_blk(xr0, xg0, xf0, filters=32, kernel_size=32, strides=1, padding='same')
    xr1, xg1, xf1 = the_blk(xr1, xg1, xf1, filters=32, kernel_size=32, strides=2, padding='same')
    xr1, xg1, xf1 = the_blk(xr1, xg1, xf1, filters=64, kernel_size=16, strides=2, padding='same')
    xr1, xg1, xf1 = the_blk(xr1, xg1, xf1, filters=64, kernel_size=16, strides=2, padding='same')
    xr2, xg2, xf2 = the_blk(xr1, xg1, xf1, filters=128, kernel_size=6, strides=2, padding='same')
    x21 = layers.GlobalAveragePooling1D()(xr2)
    x22 = layers.GlobalAveragePooling1D()(xg2)
    x23 = layers.GlobalAveragePooling1D()(xf2)
    x2 = tf.concat([x21, x22, x23], axis=-1)

    output = layers.Dense(units = class_number, activation='softmax')(x2)
    model = tf.keras.Model(inputs=in_x, outputs=output)

    return model


#%%
def normalization_processing(data):
    data_mean = data.mean()
    data_var = data.var()

    data = data - data_mean
    data = data / data_var

    return data

def wgn(x, snr):

    snr = 10**(snr/10.0)
    xpower = np.sum(x**2)/len(x)
    npower = xpower / snr

    return np.random.randn(len(x)) * np.sqrt(npower)


def add_noise(data,snr_num):

    rand_data = wgn(data, snr_num)
    data = data + rand_data

    return data
